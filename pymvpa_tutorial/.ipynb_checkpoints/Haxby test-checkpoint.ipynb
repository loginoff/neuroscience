{
 "metadata": {
  "name": "",
  "signature": "sha256:0633d593cc607addf8a52cb3f530e6688a60d63f7b1cbb99a7d78b63b6c28316"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import glob\n",
      "import numpy as np\n",
      "import nibabel as nb\n",
      "from mvpa2.datasets.mri import fmri_dataset\n",
      "from mvpa2.misc.io import SampleAttributes\n",
      "from mvpa2.datasets import vstack\n",
      "from mvpa2.mappers.detrend import PolyDetrendMapper\n",
      "from mvpa2.mappers.detrend import poly_detrend\n",
      "from mvpa2.mappers.zscore import ZScoreMapper\n",
      "from mvpa2.mappers.zscore import zscore\n",
      "from mvpa2.clfs.knn import kNN\n",
      "from mvpa2.mappers.fx import BinaryFxNode\n",
      "%pylab --no-import-all inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This block takes care of parsing the different condition onsets from the supplemental data for each run, maps these onsets (which are in seconds from the beginning of each run) onto brain volumes and stores them as sample attributes to the dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "condition_key = {\n",
      "'cond001':'house',\n",
      "'cond002':'scrambledpix',\n",
      "'cond003':'cat',\n",
      "'cond004':'shoe',\n",
      "'cond005':'bottle',\n",
      "'cond006':'scissors',\n",
      "'cond007':'chair',\n",
      "'cond008':'face',\n",
      "}\n",
      "TR=2.5\n",
      "def parse_targets(onset_path, ds):\n",
      "    targets = np.chararray(ds.shape[0],itemsize=12)\n",
      "    targets[:] = 'rest'\n",
      "    condfiles=glob.glob(os.path.join(onset_path,'cond*'))\n",
      "    for cfile in condfiles:\n",
      "        condition = os.path.basename(cfile).rstrip('.txt')\n",
      "        with open(cfile,'r') as f:\n",
      "            for line in f:\n",
      "                start, duration, weight = line.split()\n",
      "                startidx = float(start) / TR\n",
      "                endidx = (float(start)+float(duration)) / TR\n",
      "                targets[int(startidx):int(endidx)+1] = condition_key[condition]\n",
      "    ds.sa['targets']=targets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load the data for subject 1 for all 12 runs and concatenate into a single dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "boldpath = '../tutorial_data/data/sub001/BOLD/task001_run0%02d/bold.nii.gz'\n",
      "#boldpath = '../ds105/sub001/BOLD/task001_run0%02d/bold.nii.gz'\n",
      "onset_path = '../tutorial_data/data/sub001/model/model001/onsets/task001_run0%02d'\n",
      "#onset_path = '../ds105/sub001/model/model001/onsets/task001_run0%02d/'\n",
      "maskpath = '../tutorial_data/data/sub001/masks/orig/vt.nii.gz'\n",
      "ds = fmri_dataset(samples=boldpath % 1, mask=maskpath)\n",
      "parse_targets(onset_path % 1,ds)\n",
      "ds.sa['chunks']=np.zeros(len(ds))\n",
      "#ds.sa.time_coords=ds.sa.time_coords*TR\n",
      "\n",
      "for i in xrange(2,13):\n",
      "    dsi = fmri_dataset(samples=boldpath % i, mask=maskpath)\n",
      "    parse_targets(onset_path % i, dsi)\n",
      "    chunks = np.empty(len(dsi))\n",
      "    chunks[:]=i-1\n",
      "    dsi.sa['chunks']=chunks\n",
      "    #dsi.sa.time_coords=dsi.sa.time_coords*TR\n",
      "    \n",
      "    #ds=vstack((ds,dsi),0)\n",
      "    ds.append(dsi)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This block takes care of parsing the different condition onsets from the supplemental data for each run, maps these onsets (which are in seconds from the beginning of each run) onto brain volumes and stores them as sample attributes to the dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#poly_detrend(ds, polyord=1, chunks_attr='chunks')\n",
      "#detrender = PolyDetrendMapper(polyord=1, chunks_attr='chunks')\n",
      "#detrended_ds = ds.get_mapped(detrender)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# do chunkswise linear detrending on dataset\n",
      "poly_detrend(ds, polyord=1, chunks_attr='chunks', space='time_coords')\n",
      "#poly_detrend(ds, polyord=1, chunks_attr='chunks')\n",
      "\n",
      "# mark the odd and even runs\n",
      "rnames = {0: 'even', 1: 'odd'}\n",
      "ds.sa['runtype'] = [rnames[c % 2] for c in ds.sa.chunks]\n",
      "\n",
      "# compute the mean sample per condition and odd vs. even runs\n",
      "# aka \"constructive interference\"\n",
      "ds = ds.get_mapped(mean_group_sample(['targets', 'runtype']))\n",
      "\n",
      "# XXX suboptimal order: should be zscore->avg\n",
      "# but then: where is the difference between this and _alternative()?\n",
      "# zscore dataset relative to baseline ('rest') mean\n",
      "zscore(ds, param_est=('targets', ['rest']))\n",
      "\n",
      "# exclude the rest condition from the dataset\n",
      "ds = ds[ds.sa.targets != 'rest']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = kNN(k=1, dfx=one_minus_correlation, voting='majority')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.train(ds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = clf.predict(ds.samples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "split1 = ds[ds.sa.runtype=='even']\n",
      "split2 = ds[ds.sa.runtype=='odd']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.set_postproc(BinaryFxNode(mean_mismatch_error,'targets'))\n",
      "clf.train(split2)\n",
      "err=clf(split1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_raw_haxby2001_data??"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    }
   ],
   "metadata": {}
  }
 ]
}