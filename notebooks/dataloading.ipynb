{
 "metadata": {
  "name": "",
  "signature": "sha256:cfd9b33c445ece88830390220c5fd4b9761181c7c2fa797d08c17ad780f4413f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import glob\n",
      "import numpy as np\n",
      "import nibabel as nb\n",
      "import re\n",
      "from mvpa2.datasets.mri import fmri_dataset\n",
      "from mvpa2.misc.io import SampleAttributes\n",
      "from mvpa2.datasets import vstack\n",
      "from mvpa2.mappers.detrend import PolyDetrendMapper\n",
      "from mvpa2.mappers.detrend import poly_detrend\n",
      "from mvpa2.mappers.zscore import ZScoreMapper\n",
      "from mvpa2.mappers.zscore import zscore\n",
      "from mvpa2.clfs.knn import kNN\n",
      "from mvpa2.clfs.svm import LinearCSVMC\n",
      "from mvpa2.clfs.distance import one_minus_correlation\n",
      "from mvpa2.mappers.fx import BinaryFxNode\n",
      "from mvpa2.mappers.fx import mean_group_sample\n",
      "from mvpa2.generators.partition import HalfPartitioner\n",
      "from mvpa2.generators.partition import NFoldPartitioner\n",
      "from mvpa2.measures.base import CrossValidation\n",
      "%pylab --no-import-all inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_openfmri_ds(root, subject, mask=None, filterfun=None):\n",
      "    TR=3.0\n",
      "    \n",
      "    ####Define helper functions for loading different parts of the data####\n",
      "    def read_condition_keys(DSROOT):\n",
      "        maxlength = 0\n",
      "        def pick_fields(keys, line):\n",
      "            fields = line.split()\n",
      "            try:\n",
      "                keys[fields[0]][fields[1]] = ' '.join(fields[2:])\n",
      "            except KeyError:\n",
      "                keys[fields[0]]={fields[1] : ' '.join(fields[2:])}\n",
      "            return keys\n",
      "\n",
      "        with open(os.path.join(DSROOT, \n",
      "                                    'models/model001/condition_key.txt'),'r') as keyfile:\n",
      "            return reduce(pick_fields,keyfile,{})\n",
      "        \n",
      "    def parse_condition_onsets(path):\n",
      "        condfiles=glob.glob(os.path.join(path, 'cond*'))\n",
      "        timeline = []\n",
      "        for cfile in condfiles:\n",
      "            cond_name = os.path.basename(cfile).rstrip('.txt')\n",
      "            with open(cfile,'r') as cfh:\n",
      "                for line in cfh:\n",
      "                    start, duration, weight = line.split()\n",
      "                    timeline.append((float(start), float(duration), cond_name))\n",
      "        timeline.sort()\n",
      "        return timeline\n",
      "        \n",
      "    def extract_task_and_run(string):\n",
      "        m=re.search('task([0-9]+)_run([0-9]+)', string)\n",
      "        return int(m.group(1)), int(m.group(2))\n",
      "\n",
      "    def load_run(runstring):\n",
      "        ds=fmri_dataset(samples=os.path.join(root,subject,'BOLD',runstring,'bold.nii.gz'))\n",
      "        task, run = extract_task_and_run(runstring)\n",
      "\n",
      "        ds.sa['chunks'] = np.empty(len(ds))\n",
      "        ds.sa.chunks.fill(run)\n",
      "        ds.sa['task'] = np.empty(len(ds))\n",
      "        ds.sa.task.fill(task)\n",
      "        return ds\n",
      "    \n",
      "    def merge_conditions_onto_ds(ds, onsets):\n",
      "        targets = np.chararray(ds.shape[0],itemsize=17)\n",
      "        targets.fill('rest')\n",
      "        for cond in onsets:\n",
      "            start, duration, condition = cond\n",
      "            startidx = int(start/TR)\n",
      "            endidx = int((start+duration)/TR)\n",
      "            targets[startidx:endidx+1] = condition_keys['task001'][condition]\n",
      "        ds.sa['targets']=targets\n",
      "    \n",
      "    ##Actual data loading begins here\n",
      "    condition_keys = read_condition_keys(root)\n",
      "    \n",
      "    allruns = map(lambda x: os.path.basename(x),\n",
      "                glob.glob(os.path.join(root, subject,'BOLD/task*')))\n",
      "    \n",
      "    if filterfun:\n",
      "        allruns = filter(filterfun,allruns)\n",
      "    \n",
      "    alldata=[]\n",
      "    for run in allruns:\n",
      "        ds=load_run(run)\n",
      "        onsets=parse_condition_onsets(os.path.join(root,subject,'model/model001/onsets/',run))\n",
      "        merge_conditions_onto_ds(ds,onsets)\n",
      "        alldata.append(ds)\n",
      "        print ds.a.keys()\n",
      "        \n",
      "    merged = vstack(alldata)\n",
      "    merged.a.update(alldata[0].a)\n",
      "    return merged"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds=load_openfmri_ds('../ds107','sub009')\n",
      "print ds.a.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['mapper', 'imgtype', 'voxel_eldim', 'voxel_dim', 'imghdr']\n",
        "['mapper', 'imgtype', 'voxel_eldim', 'voxel_dim', 'imghdr']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['mapper', 'imgtype', 'imghdr', 'voxel_dim', 'voxel_eldim']\n"
       ]
      }
     ],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ds.a.mapper"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<FlattenMapper>\n"
       ]
      }
     ],
     "prompt_number": 116
    }
   ],
   "metadata": {}
  }
 ]
}